{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS224W - Colab 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanyun-sun/CS224w/blob/main/CS224W_Colab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Colab 2 we constructed GNN models by using PyTorch Geometric built in GCN layer, the `GCNConv`. In this Colab we will implement the **GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) and **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)) layers directly. Then we will run our models on the CORA dataset, which is a stanford citation network benchmark dataset.\n",
        "\n",
        "We will then use [DeepSNAP](https://snap.stanford.edu/deepsnap/), a Python library assisting efficient deep learning on graphs, to split the graphs in different settings and apply dataset transformations.\n",
        "\n",
        "At last, using DeepSNAP transductive link prediction split functionality, we will construct a simple GNN model on the edge property predition (link prediction) task.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
        "\n",
        "Have fun on Colab 3 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSaetj53YnT6"
      },
      "source": [
        "# Device\n",
        "You might need to use GPU for this Colab.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gOQITlCNQi"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_m9l6OYCQZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8df386a-4272-49bf-f4d7-d1e459e089e0"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.9MB 226kB/s \n",
            "\u001b[K     |████████████████████████████████| 24.3MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 5.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfgbfTjCRD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0bbf42fd-853d-4bc0-e74e-3426964bfdda"
      },
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoXlf4MtYrbz"
      },
      "source": [
        "# 1 GNN Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQy2RBfgYut4"
      },
      "source": [
        "## Implementing Layer Modules\n",
        "\n",
        "In colab 2, we implemented a network using GCN in node and graph classification tasks. However, the GCN module we used in colab 2 is from the official library. For this problem, we will provide you with a general Graph Neural Network Stack, where you'll be able to plugin your own modules of GraphSAGE and GATs. We will use our implementations to complete node classification on CORA, which is a standard citation network benchmark dataset. In this dataset, nodes correspond to documents and edges correspond to undirected citations. Each node has a class label. The node features are elements of a bag-or-words representation of a document. For the Cora dataset, there are 2708 nodes, 5429 edges, 7 prediction classes for nodes, and 1433 features per node. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ne6Gw-CT5G"
      },
      "source": [
        "## GNN Stack Module\n",
        "\n",
        "Below is the implementation for a general GNN Module that could plugin any layers, including **GraphSage**, **GAT**, etc. This module is provided for you, and you own **GraphSage** and **GAT** layers will function as components in the GNNStack Module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys8vZAFPCWWe"
      },
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
        "                                    OptTensor)\n",
        "\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "        self.emb = emb\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GraphSage':\n",
        "            return GraphSage\n",
        "        elif model_type == 'GAT':\n",
        "            # When applying GAT with num heads > 1, one needs to modify the \n",
        "            # input and output dimension of the conv layers (self.convs),\n",
        "            # to ensure that the input dim of the next layer is num heads\n",
        "            # multiplied by the output dim of the previous layer.\n",
        "            # HINT: In case you want to play with multiheads, you need to change the for-loop when builds up self.convs to be\n",
        "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
        "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "          \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        if self.emb == True:\n",
        "            return x\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syDtxjxoCZgq"
      },
      "source": [
        "## GraphSage Implementation\n",
        "\n",
        "Now let's start working on our own implementation of layers! This part is to get you familiar with how to implement Pytorch layer based on Message Passing. You will be implementing the **forward**, **message** and **aggregate** functions.\n",
        "\n",
        "Generally, the **forward** function is where the actual message passing is conducted. All logic in each iteration happens in **forward**, where we'll call **propagate** function to propagate information from neighbor nodes to central nodes.  So the general paradigm will be pre-processing -> propagate -> post-processing.\n",
        "\n",
        "Recall the process of message passing we introduced in homework 1. **propagate** further calls **message** which transforms information of neighbor nodes into messages, **aggregate** which aggregates all messages from neighbor nodes into one, and **update** which further generates the embedding for nodes in the next iteration.\n",
        "\n",
        "Our implementation is slightly variant from this, where we'll not explicitly implement **update**, but put the logic for updating nodes in **forward** function. To be more specific, after information is propagated, we can further conduct some operations on the output of **propagate**. The output of **forward** is exactly the embeddings after the current iteration.\n",
        "\n",
        "In addition, tensors passed to **propagate()** can be mapped to the respective nodes $i$ and $j$ by appending _i or _j to the variable name, .e.g. x_i and x_j. Note that we generally refer to $i$ as the central nodes that aggregates information, and refer to $j$ as the neighboring nodes, since this is the most common notation.\n",
        "\n",
        "Please find more details in the comments. One thing to note is that we're adding **skip connections** to our GraphSage. Formally, the update rule for our model is described as below:\n",
        "\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
        "\\end{equation}\n",
        "\n",
        "For simplicity, we use mean aggregations where:\n",
        "\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
        "\\end{equation}\n",
        "\n",
        "Additionally, $\\ell$-2 normalization is applied after each iteration.\n",
        "\n",
        "In order to complete the work correctly, we have to understand how the different functions interact with each other. In **propagate** we can pass in any parameters we want. For example, we pass in $x$ as an parameter:\n",
        "\n",
        "... = propagate(..., $x$=($x_{central}$, $x_{neighbor}$), ...)\n",
        "\n",
        "Here $x_{central}$ and $x_{neighbor}$ represent the features from **central** nodes and from **neighbor** nodes. If we're using the same representations from central and neighbor, then $x_{central}$ and $x_{neighbor}$ could be identical.\n",
        "\n",
        "Suppose $x_{central}$ and $x_{neighbor}$ are both of shape N * d, where N is number of nodes, and d is dimension of features.\n",
        "\n",
        "Then in message function, we can take parameters called $x\\_i$ and $x\\_j$. Usually $x\\_i$ represents \"central nodes\", and $x\\_j$ represents \"neighbor nodes\". Pay attention to the shape here: $x\\_i$ and $x\\_j$ are both of shape E * d (**not N!**). $x\\_i$ is obtained by concatenating the embeddings of central nodes of all edges through lookups from $x_{central}$ we passed in propagate. Similarly, $x\\_j$ is obtained by concatenating the embeddings of neighbor nodes of all edges through lookups from $x_{neighbor}$ we passed in propagate.\n",
        "\n",
        "Let's look at an example. Suppose we have 4 nodes, so $x_{central}$ and $x_{neighbor}$ are of shape 4 * d. We have two edges (1, 2) and (3, 0). Thus, $x\\_i$ is obtained by $[x_{central}[1]^T; x_{central}[3]^T]^T$, and $x\\_j$ is obtained by $[x_{neighbor}[2]^T; x_{neighbor}[0]^T]^T$\n",
        "\n",
        "<font color='red'>For the following questions, DON'T refer to any existing implementations online.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwG4HqCFCaOD"
      },
      "source": [
        "class GraphSage(MessagePassing):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, normalize = True,\n",
        "                 bias = False, **kwargs):  \n",
        "        super(GraphSage, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message and update functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embedding \n",
        "        #            for central node.\n",
        "        # self.lin_r is the linear transformation that you apply to aggregated \n",
        "        #            message from neighbors.\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "        self.lin_l = nn.Linear(in_channels, out_channels)\n",
        "        self.lin_r = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        self.lin_r.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any post-processing (our update rule).\n",
        "        # 1. First call propagate function to conduct the message passing.\n",
        "        #    1.1 See there for more information: \n",
        "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        #    1.2 We use the same representations for central (x_central) and \n",
        "        #        neighbor (x_neighbor) nodes, which means you'll pass x=(x, x) \n",
        "        #        to propagate.\n",
        "        # 2. Update our node embedding with skip connection.\n",
        "        # 3. If normalize is set, do L-2 normalization (defined in \n",
        "        #    torch.nn.functional)\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "        out = self.propagate(edge_index, x=(x,x), size=size)\n",
        "        out = self.lin_l(out)\n",
        "        out += self.lin_r(x)\n",
        "        if self.normalize:\n",
        "          out = F.normalize(out, dim=-1)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function here.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        out = x_j\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        # The axis along which to index number of nodes.\n",
        "        node_dim = self.node_dim\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: \n",
        "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        return torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce='mean')\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcfF3RACdLD"
      },
      "source": [
        "## GAT Implementation\n",
        "\n",
        "Attention mechanisms have become the state-of-the-art in many sequence-based tasks such as machine translation and learning sentence representations. One of the major benefits of attention-based mechanisms is their ability to focus on the most relevant parts of the input to make decisions. In this problem, we will see how attention mechanisms can be used to perform node classification of graph-structured data through the usage of Graph Attention Networks (GATs).\n",
        "\n",
        "The building block of the Graph Attention Network is the graph attention layer, which is a variant of the aggregation function . Let $N$ be the number of nodes and $F$ be the dimension of the feature vector for each node. The input to each graph attentional layer is a set of node features: $\\mathbf{h} = \\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}$\\}, $\\overrightarrow{h_i} \\in R^F$. The output of each graph attentional layer is a new set of node features, which may have a new dimension $F'$: $\\mathbf{h'} = \\{\\overrightarrow{h_1'}, \\overrightarrow{h_2'}, \\dots, \\overrightarrow{h_N'}\\}$, with $\\overrightarrow{h_i'} \\in \\mathbb{R}^{F'}$.\n",
        "\n",
        "We will now describe this transformation of the input features into higher-level features performed by each graph attention layer. First, a shared linear transformation parametrized by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ is applied to every node. Next, we perform self-attention on the nodes. We use a shared attentional mechanism:\n",
        "\\begin{equation} \n",
        "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
        "\\end{equation}\n",
        "\n",
        "This mechanism computes the attention coefficients that capture the importance of node $j$'s features to node $i$:\n",
        "\\begin{equation}\n",
        "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
        "\\end{equation}\n",
        "The most general formulation of self-attention allows every node to attend to all other nodes which drops all structural information. To utilize graph structure in the attention mechanisms, we can use masked attention. In masked attention, we only compute $e_{ij}$ for nodes $j \\in \\mathcal{N}_i$ where $\\mathcal{N}_i$ is some neighborhood of node $i$ in the graph.\n",
        "\n",
        "To easily compare coefficients across different nodes, we normalize the coefficients across $j$ using a softmax function:\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "\\end{equation}\n",
        "\n",
        "For this problem, our attention mechanism $a$ will be a single-layer feedforward neural network parametrized by a weight vector $\\overrightarrow{a} \\in \\mathbb{R}^{F'}$, followed by a LeakyReLU nonlinearity (with negative input slope 0.2). Let $\\cdot^T$ represent transposition and $||$ represent concatenation. The coefficients computed by our attention mechanism may be expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
        "\\end{equation}\n",
        "\n",
        "For the following questions, we denote $\\alpha_l = [...,\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i},...]$ and $\\alpha_r = [..., \\overrightarrow{a_r}^T \\mathbf{W_r} \\overrightarrow{h_j}, ...]$.\n",
        "\n",
        "\n",
        "At every layer of GAT, after the attention coefficients are computed for that layer, the aggregation function can be computed by a weighted sum of neighborhood messages, where weights are specified by $\\alpha_{ij}$.\n",
        "\n",
        "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
        "\n",
        "\\begin{equation}\n",
        "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
        "\\end{equation}\n",
        "\n",
        "To stabilize the learning process of self-attention, we use multi-head attention. To do this we use $K$ independent attention mechanisms, or ``heads'' compute output features as in the above equations. Then, we concatenate these output feature representations:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "\\end{equation}\n",
        "\n",
        "where $||$ is concentation, $\\alpha_{ij}^{(k)}$ are the normalized attention coefficients computed by the $k$-th attention mechanism $(a^k)$, and $\\mathbf{W}^{(k)}$ is the corresponding input linear transformation's weight matrix. Note that for this setting, $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4j45gTpCeXO"
      },
      "source": [
        "class GAT(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, heads = 2,\n",
        "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "        self.att_l = None\n",
        "        self.att_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embeddings \n",
        "        # BEFORE message passing.\n",
        "        # Pay attention to dimensions of the linear layers, since we're using \n",
        "        # multi-head attention.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        self.lin_l = nn.Linear(in_channels, heads*out_channels, bias=False)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.lin_r = self.lin_l\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
        "        # You have to deal with multi-head scenarios.\n",
        "        # Use nn.Parameter instead of nn.Linear\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "        self.att_l = Parameter(torch.Tensor(1, heads, out_channels))\n",
        "        self.att_r = Parameter(torch.Tensor(1, heads, out_channels))\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
        "        # 1. First apply linear transformation to node embeddings, and split that \n",
        "        #    into multiple heads. We use the same representations for source and\n",
        "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
        "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
        "        # 3. Call propagate function to conduct the message passing. \n",
        "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
        "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        # 4. Transform the output back to the shape of N * d.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        # [N, feat_dim] --> [N, heads*out_channel]\n",
        "        x_l, x_r = self.lin_l(x), self.lin_l(x)\n",
        "        x_l, x_r = x_l.view(-1, H, C), x_r.view(-1, H, C)\n",
        "        alpha_l = (x_l * self.att_l).sum(dim=-1)\n",
        "        alpha_r = (x_r * self.att_r).sum(dim=-1)\n",
        "        out = self.propagate(edge_index, x=(x_l, x_r),\n",
        "                             alpha=(alpha_l, alpha_r),\n",
        "                             size=size)\n",
        "        out = out.view(-1, H*C)\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function. Putting the attention in message \n",
        "        # instead of in update is a little tricky.\n",
        "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
        "        #    and apply leaky Relu.\n",
        "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
        "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
        "        # 3. Apply dropout to attention weights (alpha).\n",
        "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
        "        #    should be of shape E * H * d.\n",
        "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
        "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        # message from node j to node i\n",
        "        # x_j: [E, H, C]\n",
        "        # alpha_j, alpha_i: [E, H]\n",
        "        alpha = F.leaky_relu(alpha_j+alpha_i, self.negative_slope)\n",
        "        alpha = torch_geometric.utils.softmax(alpha, index, ptr, size_i)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "        # [E, H, C]\n",
        "        out = x_j * alpha.unsqueeze(-1)\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
        "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce='sum')\n",
        "        ############################################################################\n",
        "    \n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2dkgSuWCheU"
      },
      "source": [
        "## Building Optimizers\n",
        "\n",
        "This function has been implemented for you. **For grading purposes please use the default Adam optimizer**, but feel free to play with other types of optimizers on your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_TIQ8NPCjBP"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYdWFwYCkwY"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "Here we provide you with the functions to train and test. **Please do not modify this part for grading purposes.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tZMWRc8CmGg"
      },
      "source": [
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train(dataset, args):\n",
        "    \n",
        "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
        "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, \n",
        "                            args)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    losses = []\n",
        "    test_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            pred = pred[batch.train_mask]\n",
        "            label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          test_acc = test(test_loader, model)\n",
        "          test_accs.append(test_acc)\n",
        "        else:\n",
        "          test_accs.append(test_accs[-1])\n",
        "    return test_accs, losses\n",
        "\n",
        "def test(loader, model, is_validation=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        mask = data.val_mask if is_validation else data.test_mask\n",
        "        # node classification: only evaluate on nodes in test set\n",
        "        pred = pred[mask]\n",
        "        label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    total = 0\n",
        "    for data in loader.dataset:\n",
        "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "    return correct / total\n",
        "  \n",
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-h7jIsCns4"
      },
      "source": [
        "## Let's Start the Training!\n",
        "\n",
        "We will be working on the CORA dataset on node-level classification.\n",
        "\n",
        "This part is implemented for you. **For grading purposes, please do not modify the default parameters.** However, feel free to play with different configurations just for fun!\n",
        "\n",
        "**Submit your best accuracy and loss on Gradescope.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe9B45l9Cpz2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5d6dac8-885d-42b2-f158-a5ecd52d7398"
      },
      "source": [
        "def main():\n",
        "    for args in [\n",
        "        {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "        args = objectview(args)\n",
        "        for model in ['GraphSage', 'GAT']:\n",
        "            args.model_type = model\n",
        "\n",
        "            # Match the dimension.\n",
        "            if model == 'GAT':\n",
        "              args.heads = 2\n",
        "            else:\n",
        "              args.heads = 1\n",
        "\n",
        "            if args.dataset == 'cora':\n",
        "                dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "            else:\n",
        "                raise NotImplementedError(\"Unknown dataset\") \n",
        "            test_accs, losses = train(dataset, args) \n",
        "\n",
        "            print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Node task. test set size: 140\n",
            "Maximum accuracy: 0.726\n",
            "Minimum loss: 0.10348768532276154\n",
            "Node task. test set size: 140\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "torch.Size([10556, 2, 32])\n",
            "Maximum accuracy: 0.73\n",
            "Minimum loss: 0.02477395348250866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHELqjARZ1W5"
      },
      "source": [
        "## Question 1.1: What is the maximum accuracy you could get on test set for GraphSage? (10 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlCtBEBLMBkR"
      },
      "source": [
        "## Question 1.2: What is the maximum accuracy you could get on test set for GAT? (10 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# 2 DeepSNAP Basics\n",
        "\n",
        "In previous Colabs we used both of graph class (NetworkX) and tensor (PyG) representations of graphs separately. The graph class `nx.Graph` provides rich analysis and manipulation functionalities, such as the clustering coefficient and PageRank. To feed the graph into the model, we need to transform the graph into tensor representations including edge tensor `edge_index` and node attributes tensors `x` and `y`. But only using tensors (as the graphs formatted in PyG `datasets` and `data`) will make many graph manipulations and analysis less efficient and harder. So, in this Colab we will use DeepSNAP which combines both representations and offers a full pipeline for GNN training / validation / testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "In general, [DeepSNAP](https://github.com/snap-stanford/deepsnap) is a Python library to assist efficient deep learning on graphs. DeepSNAP features in its support for flexible graph manipulation, standard pipeline, heterogeneous graphs and simple API.\n",
        "\n",
        "1. DeepSNAP is easy to be used for the sophisticated graph manipulations, such as feature computation, pretraining, subgraph extraction etc. during/before the training.\n",
        "2. In most frameworks, standard pipelines for node, edge, link, graph-level tasks under inductive or transductive settings are left to the user to code. In practice, there are additional design choices involved (such as how to split dataset for link prediction). DeepSNAP provides such a standard pipeline that greatly saves repetitive coding efforts, and enables fair comparision for models.\n",
        "3. Many real-world graphs are heterogeneous graphs. But packages support for heterogeneous graphs, including data storage and flexible message passing, is lacking. DeepSNAP provides an efficient and flexible heterogeneous graph that supports both the node and edge heterogeneity.\n",
        "\n",
        "[DeepSNAP](https://github.com/snap-stanford/deepsnap) is a newly released project and it is still under development. If you find any bugs or have any improvement ideas, feel free to raise issues or create pull requests on the GitHub directly :)\n",
        "\n",
        "In this Colab, we will focus on DeepSNAP graph manipulations and splitting settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20SvvngpQmmQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfbBVFmAQlwz"
      },
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def visualize(G, color_map=None, seed=123):\n",
        "  if color_map is None:\n",
        "    color_map = '#c92506'\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  nodes = nx.draw_networkx_nodes(G, pos=nx.spring_layout(G, seed=seed), \\\n",
        "                                 label=None, node_color=color_map, node_shape='o', node_size=150)\n",
        "  edges = nx.draw_networkx_edges(G, pos=nx.spring_layout(G, seed=seed), alpha=0.5)\n",
        "  if color_map is not None:\n",
        "    plt.scatter([],[], c='#c92506', label='Nodes with label 0', edgecolors=\"black\", s=140)\n",
        "    plt.scatter([],[], c='#fcec00', label='Nodes with label 1', edgecolors=\"black\", s=140)\n",
        "    plt.legend(prop={'size': 13}, handletextpad=0)\n",
        "  nodes.set_edgecolor('black')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "## DeepSNAP Graph\n",
        "\n",
        "The `deepsnap.graph.Graph` class is the core class of DeepSNAP. It not only represents a graph in tensor format but also references to a graph object from graph manipulation package.\n",
        "\n",
        "Currently DeepSNAP supports [NetworkX](https://networkx.org/) and [Snap.py](https://snap.stanford.edu/snappy/doc/index.html) as the back end graph manipulation package.\n",
        "\n",
        "In this Colab, we will use the NetworkX as the back end graph manipulation package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ispq_lIoJl_z"
      },
      "source": [
        "Lets first try to convert a simple random NetworkX graph to a DeepSNAP graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT5qca3x6XpG"
      },
      "source": [
        "num_nodes = 100\n",
        "p = 0.05\n",
        "seed = 100\n",
        "\n",
        "# Generate a networkx random graph\n",
        "G = nx.gnp_random_graph(num_nodes, p, seed=seed)\n",
        "\n",
        "# Generate some random node features and labels\n",
        "node_feature = {node : torch.rand([5, ]) for node in G.nodes()}\n",
        "node_label = {node : torch.randint(0, 2, ()) for node in G.nodes()}\n",
        "\n",
        "# Set the random features and labels to G\n",
        "nx.set_node_attributes(G, node_feature, name='node_feature')\n",
        "nx.set_node_attributes(G, node_label, name='node_label')\n",
        "\n",
        "# Print one node example\n",
        "for node in G.nodes(data=True):\n",
        "  print(node)\n",
        "  break\n",
        "\n",
        "color_map = ['#c92506' if node[1]['node_label'].item() == 0 else '#fcec00' for node in G.nodes(data=True)]\n",
        "\n",
        "# Visualize the graph\n",
        "visualize(G, color_map=color_map)\n",
        "\n",
        "# Transform the networkx graph into the deepsnap graph\n",
        "graph = Graph(G)\n",
        "\n",
        "# Print out the general deepsnap graph information\n",
        "print(graph)\n",
        "\n",
        "# DeepSNAP will convert node attributes to tensors\n",
        "# Notice the type of tensors\n",
        "print(\"Node feature (node_feature) has shape {} and type {}\".format(graph.node_feature.shape, graph.node_feature.dtype))\n",
        "print(\"Node label (node_label) has shape {} and type {}\".format(graph.node_label.shape, graph.node_label.dtype))\n",
        "\n",
        "# DeepSNAP will also generate the edge_index tensor\n",
        "print(\"Edge index (edge_index) has shape {} and type {}\".format(graph.edge_index.shape, graph.edge_index.dtype))\n",
        "\n",
        "# Different from only storing tensors, deepsnap graph also references to the networkx graph\n",
        "# We will discuss why the reference will be helpful later\n",
        "print(\"The DeepSNAP graph has {} as the internal manupulation graph\".format(type(graph.G)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNMbc307KOQD"
      },
      "source": [
        "In DeepSNAP we have three levels of attributes. In this example, we have the **node level** attributes including `node_feature` and `node_label`. The other two levels of attributes are graph and edge attributes. The usage is similar to the node level one except that the feature becomes `edge_feature` or `graph_feature` and label becomes `edge_label` or `graph_label` etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Xz58_Da0qL"
      },
      "source": [
        "Similar to the NetworkX graph, we can easily get some basic information of the graph through class properties directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLo4zWAoeg6S"
      },
      "source": [
        "# Number of nodes\n",
        "print(\"The random graph has {} nodes\".format(graph.num_nodes))\n",
        "\n",
        "# Number of edges\n",
        "print(\"The random graph has {} edges\".format(graph.num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po7IaRmwblI5"
      },
      "source": [
        "DeepSNAP also provides functions that can automatically transform the PyG datasets into a list of DeepSNAP graphs.\n",
        "\n",
        "Here we transform the CORA dataset into a list of DeepSNAP graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFkg2kCgcFwR"
      },
      "source": [
        "root = './tmp/cora'\n",
        "name = 'Cora'\n",
        "\n",
        "# The Cora dataset\n",
        "pyg_dataset= Planetoid(root, name)\n",
        "\n",
        "# PyG dataset to a list of deepsnap graphs\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Get the first deepsnap graph (CORA only has one graph)\n",
        "graph = graphs[0]\n",
        "print(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "## Question 2.1: What is the number of classes and number of features in the CORA graph? (5 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iF_Kyqr_JbY"
      },
      "source": [
        "def get_num_node_classes(graph):\n",
        "  # TODO: Implement this function that takes a deepsnap graph object\n",
        "  # and return the number of node classes of that graph.\n",
        "\n",
        "  num_node_classes = 0\n",
        "\n",
        "  ############# Your code here #############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful\n",
        "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  return num_node_classes\n",
        "\n",
        "def get_num_node_features(graph):\n",
        "  # TODO: Implement this function that takes a deepsnap graph object\n",
        "  # and return the number of node features of that graph.\n",
        "\n",
        "  num_node_features = 0\n",
        "\n",
        "  ############# Your code here #############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful\n",
        "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  return num_node_features\n",
        "\n",
        "num_node_classes = get_num_node_classes(graph)\n",
        "num_node_features = get_num_node_features(graph)\n",
        "print(\"{} has {} classes\".format(name, num_node_classes))\n",
        "print(\"{} has {} features\".format(name, num_node_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "## DeepSNAP Dataset\n",
        "\n",
        "Now, lets talk about DeepSNAP dataset. A `deepsnap.dataset.GraphDataset` contains a list of `deepsnap.graph.Graph` objects. In addition to list of graphs, you can also specify what task the dataset will be used on, such as node level task (`task=node`), edge level task (`task=link_pred`) and graph level task (`task=graph`).\n",
        "\n",
        "It also contains many other useful parameters during initialization and other functinoalities. If you are interested, you can take a look at the [documentation](https://snap.stanford.edu/deepsnap/modules/dataset.html#deepsnap-graphdataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSidf9E0hn2s"
      },
      "source": [
        "Lets now use COX2 dataset which contains a list of graphs and specify the task to `graph` when we initialize the DeepSNAP dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4kqUldyoaS_"
      },
      "source": [
        "root = './tmp/cox2'\n",
        "name = 'COX2'\n",
        "\n",
        "# Load the dataset through PyG\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "\n",
        "# Convert to a list of deepsnap graphs\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Convert list of deepsnap graphs to deepsnap dataset with specified task=graph\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 2.2: What is the label of the graph (index 100 in the COX2 dataset)? (5 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIis9oTZAfs3"
      },
      "source": [
        "def get_graph_class(dataset, idx):\n",
        "  # TODO: Implement this function that takes a deepsnap dataset object,\n",
        "  # the index of the graph in the dataset, and returns the class/label \n",
        "  # of the graph (in integer).\n",
        "\n",
        "  label = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. The label refers to the graph-level attribute\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return label\n",
        "\n",
        "graph_0 = dataset[0]\n",
        "print(graph_0)\n",
        "idx = 100\n",
        "label = get_graph_class(dataset, idx)\n",
        "print('Graph with index {} has label {}'.format(idx, label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "## Question 2.3: What is the number of edges for the graph (index 200 in the COX2 dataset)? (5 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5m2DOfhBtWv"
      },
      "source": [
        "def get_graph_num_edges(dataset, idx):\n",
        "  # TODO: Implement this function that takes a deepsnap dataset object,\n",
        "  # the index of the graph in dataset, and returns the number of \n",
        "  # edges in the graph (in integer).\n",
        "\n",
        "  num_edges = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 lines of code)\n",
        "  ## Note\n",
        "  ## 1. You can use the class property directly\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return num_edges\n",
        "\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(dataset, idx)\n",
        "print('Graph with index {} has {} edges'.format(idx, num_edges))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# 3 DeepSNAP Advanced\n",
        "\n",
        "We have learned the basic use of DeepSNAP graph and dataset :)\n",
        "\n",
        "Lets move on to some more advanced functionalities.\n",
        "\n",
        "In this section we will use DeepSNAP for faeture computation and transductive/inductive splittings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5fsGBLY8cxa"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-jgRLiQ8cSj"
      },
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Data Split in Graphs\n",
        "\n",
        "Data splitting in graphs can be much harder than that in CV or NLP.\n",
        "\n",
        "In general, the data splitting in graphs can be divided into two settings, **inductive** and **transductive**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9KG_MhqsWBp"
      },
      "source": [
        "## Inductive Split\n",
        "\n",
        "As what we have learned in the lecture, inductive setting will split multiple graphs into each training/valiation and test sets.\n",
        "\n",
        "Here is an example of DeepSNAP inductive splitting for a list of graphs in the graph level task (graph classification etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpc6bTm3GF02"
      },
      "source": [
        "root = './tmp/cox2'\n",
        "name = 'COX2'\n",
        "\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Here we specify the task as graph-level task such as graph classification\n",
        "task = 'graph'\n",
        "dataset = GraphDataset(graphs, task=task)\n",
        "\n",
        "# Specify transductive=False (inductive)\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=False, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(\"COX2 train dataset: {}\".format(dataset_train))\n",
        "print(\"COX2 validation dataset: {}\".format(dataset_val))\n",
        "print(\"COX2 test dataset: {}\".format(dataset_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWKQwa4WsgQp"
      },
      "source": [
        "## Transductive Split\n",
        "\n",
        "In transductive setting, the training /validation / test sets are on the same graph.\n",
        "\n",
        "Here we transductively split the CORA graph in the node level task. \n",
        "\n",
        "(Notice that in DeepSNAP default setting the split is random, but you can also make a fixed split by specifying `fixed_split=True` when loading the dataset from PyG or changing the `node_label_index` directly)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5OdxSg4sfyR"
      },
      "source": [
        "root = './tmp/cora'\n",
        "name = 'Cora'\n",
        "\n",
        "pyg_dataset = Planetoid(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Here we specify the task as node-level task such as node classification\n",
        "task = 'node'\n",
        "\n",
        "dataset = GraphDataset(graphs, task=task)\n",
        "\n",
        "# Specify we want the transductive splitting\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(\"Cora train dataset: {}\".format(dataset_train))\n",
        "print(\"Cora validation dataset: {}\".format(dataset_val))\n",
        "print(\"Cora test dataset: {}\".format(dataset_test))\n",
        "\n",
        "print(\"Original Cora has {} nodes\".format(dataset.num_nodes[0]))\n",
        "\n",
        "# The nodes in each set can be find in node_label_index\n",
        "print(\"After the split, Cora has {} training nodes\".format(dataset_train[0].node_label_index.shape[0]))\n",
        "print(\"After the split, Cora has {} validation nodes\".format(dataset_val[0].node_label_index.shape[0]))\n",
        "print(\"After the split, Cora has {} test nodes\".format(dataset_test[0].node_label_index.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ePKgM00lGE"
      },
      "source": [
        "## Edge Level Split\n",
        "\n",
        "Compared to the node and graph level splitting, edge level splitting is a little bit tricky ;)\n",
        "\n",
        "Usually in edge level splitting, we need to sample negative edges, split positive edges into different datasets, split training edges into message passing edges and supervision edges, and resample the negative edges during the training etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnzISX5RoiR6"
      },
      "source": [
        "### All Mode\n",
        "\n",
        "Now lets start with a simpler edge level splitting mode, the `edge_train_mode=\"all\"` mode in DeepSNAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D104xO6137n"
      },
      "source": [
        "root = './tmp/cora'\n",
        "name = 'Cora'\n",
        "\n",
        "pyg_dataset = Planetoid(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Specify task as link_pred for edge-level task\n",
        "task = 'link_pred'\n",
        "\n",
        "# Specify the train mode, \"all\" mode is default for deepsnap dataset\n",
        "edge_train_mode = \"all\"\n",
        "\n",
        "dataset = GraphDataset(graphs, task=task, edge_train_mode=edge_train_mode)\n",
        "\n",
        "# Transductive link prediction split\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(\"Cora train dataset: {}\".format(dataset_train))\n",
        "print(\"Cora validation dataset: {}\".format(dataset_val))\n",
        "print(\"Cora test dataset: {}\".format(dataset_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GscopwOXC_Y7"
      },
      "source": [
        "In DeepSNAP, the indices of supervision edges are stored in `edge_label_index` tensor and the corresponding edge labels are stored in `edge_label` tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJF8fZnA2eLR"
      },
      "source": [
        "print(\"Original Cora graph has {} edges\".format(dataset[0].num_edges))\n",
        "print(\"Because Cora graph is undirected, the original edge_index has shape {}\".format(dataset[0].edge_index.shape))\n",
        "\n",
        "print(\"The training set has message passing edge index shape {}\".format(dataset_train[0].edge_index.shape))\n",
        "print(\"The training set has supervision edge index shape {}\".format(dataset_train[0].edge_label_index.shape))\n",
        "\n",
        "print(\"The validation set has message passing edge index shape {}\".format(dataset_val[0].edge_index.shape))\n",
        "print(\"The validation set has supervision edge index shape {}\".format(dataset_val[0].edge_label_index.shape))\n",
        "\n",
        "print(\"The test set has message passing edge index shape {}\".format(dataset_test[0].edge_index.shape))\n",
        "print(\"The test set has supervision edge index shape {}\".format(dataset_test[0].edge_label_index.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BX-I_oEKQX"
      },
      "source": [
        "We can see that both training and validation sets have the same message passing edges (`edge_index`) in the `all` mode. Also, in training set, the postive supervision edges (`edge_label_index`) are same with the message passing edges. However, in the test set the message passing edges are the combination of message passing edges from training and validation sets.\n",
        "\n",
        "Notice that the `edge_label` and `edge_label_index` have included the negative edges (default number of negative edges is same with the number of positive edges).\n",
        "\n",
        "Now, lets implement a function that checks whether two edge index tensors are disjoint and explore more edge splitting properties by using that function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOZHDskbAKN6"
      },
      "source": [
        "## Question 3.1 - 3.5: Implement the function that checks whether two edge_index tensors are disjoint. Then answer the True/False questions below. (5 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgRYdyPp8EmO"
      },
      "source": [
        "def edge_indices_disjoint(edge_index_1, edge_index_2):\n",
        "  # TODO: Implement this function that takes two edge index tensors,\n",
        "  # and returns whether these two edge index tensors are disjoint.\n",
        "  disjoint = None\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~5 lines of code)\n",
        "  ## Note\n",
        "  ## 1. Here disjoint means that there is no single edge belongs to either edge index tensors\n",
        "  ## 2. You do not need to consider the undirected case. For example, if edge_index_1 contains\n",
        "  ## edge (a, b) and edge_index_2 contains edge (b, a). We will treat them as disjoint in this\n",
        "  ## function.\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return disjoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL4ASIDDEIUf"
      },
      "source": [
        "num_train_edges = dataset_train[0].edge_label_index.shape[1] // 2\n",
        "train_pos_edge_index = dataset_train[0].edge_label_index[:, :num_train_edges]\n",
        "train_neg_edge_index = dataset_train[0].edge_label_index[:, num_train_edges:]\n",
        "print(\"3.1 Training (supervision) positve and negative edges are disjoint = {}\"\\\n",
        "        .format(edge_indices_disjoint(train_pos_edge_index, train_neg_edge_index)))\n",
        "\n",
        "num_val_edges = dataset_val[0].edge_label_index.shape[1] // 2\n",
        "val_pos_edge_index = dataset_val[0].edge_label_index[:, :num_val_edges]\n",
        "val_neg_edge_index = dataset_val[0].edge_label_index[:, num_val_edges:]\n",
        "print(\"3.2 Validation (supervision) positve and negative edges are disjoint = {}\"\\\n",
        "        .format(edge_indices_disjoint(val_pos_edge_index, val_neg_edge_index)))\n",
        "\n",
        "num_test_edges = dataset_test[0].edge_label_index.shape[1] // 2\n",
        "test_pos_edge_index = dataset_test[0].edge_label_index[:, :num_test_edges]\n",
        "test_neg_edge_index = dataset_test[0].edge_label_index[:, num_test_edges:]\n",
        "print(\"3.3 Test (supervision) positve and negative edges are disjoint = {}\"\\\n",
        "        .format(edge_indices_disjoint(test_pos_edge_index, test_neg_edge_index)))\n",
        "\n",
        "print(\"3.4 Test (supervision) positve and validation (supervision) positve edges are disjoint = {}\"\\\n",
        "        .format(edge_indices_disjoint(test_pos_edge_index, val_pos_edge_index)))\n",
        "print(\"3.5 Validation (supervision) positve and training (supervision) positve edges are disjoint = {}\"\\\n",
        "        .format(edge_indices_disjoint(val_pos_edge_index, train_pos_edge_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLoVN5ZBTuA"
      },
      "source": [
        "### Disjoint Mode\n",
        "\n",
        "Now lets look at a relatively more complex transductive edge split setting, which is the `edge_train_mode=\"disjoint\"` mode in DeepSNAP (also the transductive link prediction splitting talked in the lecture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rqzfb-0BTBm"
      },
      "source": [
        "edge_train_mode = \"disjoint\"\n",
        "\n",
        "dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=edge_train_mode)\n",
        "orig_edge_index = dataset[0].edge_index\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(\n",
        "    transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "train_message_edge_index = dataset_train[0].edge_index\n",
        "train_sup_edge_index = dataset_train[0].edge_label_index\n",
        "val_sup_edge_index = dataset_val[0].edge_label_index\n",
        "test_sup_edge_index = dataset_test[0].edge_label_index\n",
        "\n",
        "print(\"The edge index of original graph has shape: {}\".format(orig_edge_index.shape))\n",
        "print(\"The edge index of training message edges has shape: {}\".format(train_message_edge_index.shape))\n",
        "print(\"The edge index of training supervision edges has shape: {}\".format(train_sup_edge_index.shape))\n",
        "print(\"The edge index of validation message edges has shape: {}\".format(dataset_val[0].edge_index.shape))\n",
        "print(\"The edge index of validation supervision edges has shape: {}\".format(val_sup_edge_index.shape))\n",
        "print(\"The edge index of test message edges has shape: {}\".format(dataset_test[0].edge_index.shape))\n",
        "print(\"The edge index of test supervision edges has shape: {}\".format(test_sup_edge_index.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUkBhiJNciol"
      },
      "source": [
        "You can see that the training / validation message passing edges and training supervision edges are splitted differently in those two modes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WKfRjqAJHtK"
      },
      "source": [
        "### Resample Negative Edges\n",
        "\n",
        "During each training iteration, we usually need to resample the negative edges.\n",
        "\n",
        "Below we print the training and validation sets negative edges in two training iterations.\n",
        "\n",
        "You should find that the negative edges in training set will be resampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMEbnx63JHWj"
      },
      "source": [
        "dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=\"disjoint\")\n",
        "datasets = {}\n",
        "follow_batch = []\n",
        "datasets['train'], datasets['val'], datasets['test'] = dataset.split(\n",
        "    transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "dataloaders = {\n",
        "  split: DataLoader(\n",
        "    ds, collate_fn=Batch.collate(follow_batch),\n",
        "    batch_size=1, shuffle=(split=='train')\n",
        "  )\n",
        "  for split, ds in datasets.items()\n",
        "}\n",
        "neg_edges_1 = None\n",
        "for batch in dataloaders['train']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
        "  print(\"First iteration training negative edges:\")\n",
        "  print(neg_edges_1)\n",
        "  break\n",
        "neg_edges_2 = None\n",
        "for batch in dataloaders['train']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
        "  print(\"Second iteration training negative edges:\")\n",
        "  print(neg_edges_2)\n",
        "  break\n",
        "\n",
        "neg_edges_1 = None\n",
        "for batch in dataloaders['val']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
        "  print(\"First iteration validation negative edges:\")\n",
        "  print(neg_edges_1)\n",
        "  break\n",
        "neg_edges_2 = None\n",
        "for batch in dataloaders['val']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
        "  print(\"Second iteration validation negative edges:\")\n",
        "  print(neg_edges_2)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEzqh7wEdrh0"
      },
      "source": [
        "If you are interested in more graph splitting settings, please refer to the DeepSNAP dataset [documentation](https://snap.stanford.edu/deepsnap/modules/dataset.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkrYyeSUI_9_"
      },
      "source": [
        "## Graph Transformation and Feature Computation\n",
        "\n",
        "The other DeepSNAP core functionality is graph transformation / feature computation.\n",
        "\n",
        "In DeepSNAP, we divide graph transformation / feature computation into two different types. One is the transformation before training (transform the whole dataset before training directly) and another one is the transformation during training (transform batches of graphs).\n",
        "\n",
        "Here is an example that uses NetworkX back end to calculate the PageRank value and update the value to tensors before the training (transform the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnAVbZINLZ4I"
      },
      "source": [
        "def pagerank_transform_fn(graph):\n",
        "\n",
        "  # Get the referenced networkx graph\n",
        "  G = graph.G\n",
        "\n",
        "  # Calculate the pagerank by using networkx\n",
        "  pr = nx.pagerank(G)\n",
        "\n",
        "  # Transform the pagerank values to tensor\n",
        "  pr_feature = torch.tensor([pr[node] for node in range(graph.num_nodes)], dtype=torch.float32)\n",
        "  pr_feature = pr_feature.view(graph.num_nodes, 1)\n",
        "\n",
        "  # Concat the pagerank values to the node feature\n",
        "  graph.node_feature = torch.cat([graph.node_feature, pr_feature], dim=-1)\n",
        "\n",
        "root = './tmp/cox2'\n",
        "name = 'COX2'\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "print(\"Number of features before transformation: {}\".format(dataset.num_node_features))\n",
        "dataset.apply_transform(pagerank_transform_fn, update_tensor=False)\n",
        "print(\"Number of features after transformation: {}\".format(dataset.num_node_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHByE87SQkUw"
      },
      "source": [
        "## Question 3.6: Implement the transformation below and report the clustering coefficient of the node (index 3) of the graph (index 406) in the COX2 dataset. Rounded the answer to two decimal places. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNEjfOZRNjYb"
      },
      "source": [
        "def cluster_transform_fn(graph):\n",
        "  # TODO: Implement this function that takes an deepsnap graph object,\n",
        "  # transform the graph by adding nodes clustering coefficient into the \n",
        "  # graph.node_feature\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~5 lines of code)\n",
        "  ## Note\n",
        "  ## 1. Compute the clustering coefficient value for each node and\n",
        "  ## concat them to the last dimension of graph.node_feature\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "root = './cox2'\n",
        "name = 'COX2'\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "\n",
        "# Transform the dataset\n",
        "dataset.apply_transform(cluster_transform_fn, update_tensor=False)\n",
        "\n",
        "node_idx = 3\n",
        "graph_idx = 406\n",
        "node_feature = dataset[graph_idx].node_feature\n",
        "\n",
        "print(\"The node has clustering coefficient: {}\".format(round(node_feature[node_idx][-1].item(), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P5Ig7XaPYzp"
      },
      "source": [
        "Apart from transforming the dataset, DeepSNAP can also transform the graph (usually the `deepsnap.batch.Batch`) during each training iteration.\n",
        "\n",
        "Also, DeepSNAP supports the synchronization of the transformation between the referenced graph objects and tensor representations. For example, you can just update the NetworkX graph object in the transform function, and by specifying `update_tensor=True` the internal tensor representations will be automatically updated.\n",
        "\n",
        "For more information, please refer to the DeepSNAP [documentation](https://snap.stanford.edu/deepsnap/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-YLYMLFQYqp"
      },
      "source": [
        "# 4 Edge Level Prediction\n",
        "\n",
        "From last section, we know how DeepSNAP transductive split the edges in the link prediction task.\n",
        "\n",
        "Now lets use DeepSNAP and PyG together to implement a edge level prediction (link prediction) model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrKCNtvERypQ"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class LinkPredModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.2):\n",
        "        super(LinkPredModel, self).__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
        "\n",
        "        self.loss_fn = None\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note\n",
        "        ## 1. Initialize the loss function to BCEWithLogitsLoss\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        node_feature, edge_index, edge_label_index = batch.node_feature, batch.edge_index, batch.edge_label_index\n",
        "        \n",
        "        ############# Your code here #############\n",
        "        ## (~6 line of code)\n",
        "        ## Note\n",
        "        ## 1. Feed the node feature into the first conv layer\n",
        "        ## 2. Add a ReLU after the first conv layer\n",
        "        ## 3. Add dropout after the ReLU (with probability self.dropout)\n",
        "        ## 4. Feed the output to the second conv layer\n",
        "        ## 5. Select the embeddings of the source nodes and destination nodes\n",
        "        ## by using the edge_label_index and compute the similarity of each pair\n",
        "        ## by dot product\n",
        "\n",
        "        \n",
        "        ##########################################\n",
        "\n",
        "        return pred\n",
        "    \n",
        "    def loss(self, pred, link_label):\n",
        "        return self.loss_fn(pred, link_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuKbGFOu1Ka8"
      },
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "def train(model, dataloaders, optimizer, args):\n",
        "    val_max = 0\n",
        "    best_model = model\n",
        "\n",
        "    for epoch in range(1, args[\"epochs\"]):\n",
        "        for i, batch in enumerate(dataloaders['train']):\n",
        "            \n",
        "            batch.to(args[\"device\"])\n",
        "\n",
        "            ############# Your code here #############\n",
        "            ## (~6 lines of code)\n",
        "            ## Note\n",
        "            ## 1. Zero grad the optimizer\n",
        "            ## 2. Compute loss and backpropagate\n",
        "            ## 3. Update the model parameters\n",
        "\n",
        "\n",
        "            ##########################################\n",
        "\n",
        "            log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Loss: {}'\n",
        "            score_train = test(model, dataloaders['train'], args)\n",
        "            score_val = test(model, dataloaders['val'], args)\n",
        "            score_test = test(model, dataloaders['test'], args)\n",
        "\n",
        "            print(log.format(epoch, score_train, score_val, score_test, loss.item()))\n",
        "            if val_max < score_val:\n",
        "                val_max = score_val\n",
        "                best_model = copy.deepcopy(model)\n",
        "    return best_model\n",
        "\n",
        "def test(model, dataloader, args):\n",
        "    model.eval()\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    ############# Your code here #############\n",
        "    ## (~5 lines of code)\n",
        "    ## Note\n",
        "    ## 1. Loop through batches in the dataloader\n",
        "    ## 2. Feed the batch to the model\n",
        "    ## 3. Feed the model output to sigmoid\n",
        "    ## 4. Compute the ROC-AUC score by using sklearn roc_auc_score function\n",
        "    ## 5. Edge labels are stored in batch.edge_label\n",
        "\n",
        "    \n",
        "    ##########################################\n",
        " \n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTKWYX1b33V3"
      },
      "source": [
        "# Please don't change any parameters\n",
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_dim\" : 128,\n",
        "    \"epochs\" : 200,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klw_xYnE27xQ"
      },
      "source": [
        "pyg_dataset = Planetoid('./tmp/cora', 'Cora')\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "dataset = GraphDataset(\n",
        "        graphs,\n",
        "        task='link_pred',\n",
        "        edge_train_mode=\"disjoint\"\n",
        "    )\n",
        "datasets = {}\n",
        "datasets['train'], datasets['val'], datasets['test']= dataset.split(\n",
        "            transductive=True, split_ratio=[0.85, 0.05, 0.1])\n",
        "input_dim = datasets['train'].num_node_features\n",
        "num_classes = datasets['train'].num_edge_labels\n",
        "\n",
        "model = LinkPredModel(input_dim, args[\"hidden_dim\"], num_classes).to(args[\"device\"])\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "dataloaders = {split: DataLoader(\n",
        "            ds, collate_fn=Batch.collate([]),\n",
        "            batch_size=1, shuffle=(split=='train'))\n",
        "            for split, ds in datasets.items()}\n",
        "best_model = train(model, dataloaders, optimizer, args)\n",
        "log = \"Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\"\n",
        "best_train_roc = test(best_model, dataloaders['train'], args)\n",
        "best_val_roc = test(best_model, dataloaders['val'], args)\n",
        "best_test_roc = test(best_model, dataloaders['test'], args)\n",
        "print(log.format(best_train_roc, best_val_roc, best_test_roc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5brlsKElP0_"
      },
      "source": [
        "## Question 4: What is the maximum ROC-AUC score you could get for the best_model on test set? (13 points)\n",
        "\n",
        "Submit your answers on Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "In order to get credit, you must go submit your answers on Gradescope.\n",
        "\n",
        "Also, you need to submit the `ipynb` file of Colab 3, by clicking `File` and `Download .ipynb`. Please make sure that your output of each cell is available in your `ipynb` file."
      ]
    }
  ]
}